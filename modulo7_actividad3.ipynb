{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0tg+N2hMkfxaKGUVXeTD4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avalosmarco/portafolioMarco/blob/main/modulo7_actividad3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. **Configuración del entorno**:\n",
        "\n",
        "*   Crear un entorno de Apache Spark local o en la nube.\n",
        "*   Documentar los pasos seguidos para la configuración."
      ],
      "metadata": {
        "id": "wMCvUYXcqLqB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1so7CsHuQRh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d5f7e788-83ed-44c7-de69-dc8c74a39adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Spark ya está disponible en el sistema\n",
            "✅ Spark listo\n"
          ]
        }
      ],
      "source": [
        "# Instalando SPARK\n",
        "try:\n",
        "    import pyspark\n",
        "    print(\"✅ Spark ya está disponible en el sistema\")\n",
        "except ImportError:\n",
        "    print(\"⚠️  Instalando Spark...\")\n",
        "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "    !pip install -q pyspark==3.5.0 findspark\n",
        "    print(\"✅ Spark instalado\")\n",
        "\n",
        "# Configurar e inicializar\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "print(\"✅ Spark listo\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda de verificación\n",
        "print(\"1. ¿Está findspark instalado y disponible?\")\n",
        "try:\n",
        "    import findspark\n",
        "    print(\"   ✅ Sí, findspark está instalado.\")\n",
        "except ImportError:\n",
        "    print(\"   ❌ No, findspark no está instalado.\")\n",
        "\n",
        "print(\"\\n2. ¿Cuál es la ruta de SPARK_HOME?\")\n",
        "import os\n",
        "spark_home = os.environ.get(\"SPARK_HOME\", \"No está configurada\")\n",
        "print(f\"   SPARK_HOME = {spark_home}\")\n",
        "\n",
        "print(\"\\n3. Información de la sesión de Spark activa:\")\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "print(f\"   Versión de Spark: {spark.version}\")\n",
        "print(f\"   URL de la interfaz web: {spark.sparkContext.uiWebUrl}\")\n",
        "print(f\"   Modo de ejecución: {spark.sparkContext.master}\")\n",
        "\n",
        "print(\"\\n4. ¡Prueba final! Ejecutando un cálculo simple con Spark:\")\n",
        "data = [1, 2, 3, 4, 5]\n",
        "distData = spark.sparkContext.parallelize(data)\n",
        "suma = distData.sum()\n",
        "print(f\"   La suma de {data} es: {suma}\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZK4H7UkpRW2",
        "outputId": "eef11b39-7027-4603-d902-a557d6cd878c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ¿Está findspark instalado y disponible?\n",
            "   ✅ Sí, findspark está instalado.\n",
            "\n",
            "2. ¿Cuál es la ruta de SPARK_HOME?\n",
            "   SPARK_HOME = /content/spark-3.5.0-bin-hadoop3\n",
            "\n",
            "3. Información de la sesión de Spark activa:\n",
            "   Versión de Spark: 3.5.0\n",
            "   URL de la interfaz web: http://26b5c38cb517:4040\n",
            "   Modo de ejecución: local[*]\n",
            "\n",
            "4. ¡Prueba final! Ejecutando un cálculo simple con Spark:\n",
            "   La suma de [1, 2, 3, 4, 5] es: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. **Obtención de datos**:\n",
        "\n",
        "\n",
        "*   Elegir una API pública (por ejemplo, OpenWeather, CoinGecko, etc.) para obtener datos en formato JSON o CSV\n",
        "*   Extraer y cargar los datos en Apache Spark."
      ],
      "metadata": {
        "id": "fbQo7X7rqlNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# URL de la API de CoinGecko para obtener los mercados de cripto\n",
        "url = \"https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc&per_page=10&page=1&sparkline=false\"\n",
        "\n",
        "# Hacer la solicitud a la API\n",
        "response = requests.get(url)\n",
        "data = response.json() # Decodificar la respuesta JSON\n",
        "\n",
        "# Convertir los datos JSON a un DataFrame de Pandas para facilitar su manejo\n",
        "pandas_df = pd.DataFrame(data)\n",
        "\n",
        "# Ahora, mostramos las primeras filas del DataFrame de Pandas para verificar\n",
        "print(\"Datos obtenidos de la API (Pandas DataFrame):\")\n",
        "print(pandas_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyCETZXKQ6d5",
        "outputId": "74eaaacd-c2cc-402a-f8d2-3a05532596c9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos obtenidos de la API (Pandas DataFrame):\n",
            "            id symbol      name  \\\n",
            "0      bitcoin    btc   Bitcoin   \n",
            "1     ethereum    eth  Ethereum   \n",
            "2       ripple    xrp       XRP   \n",
            "3       tether   usdt    Tether   \n",
            "4  binancecoin    bnb       BNB   \n",
            "\n",
            "                                               image  current_price  \\\n",
            "0  https://coin-images.coingecko.com/coins/images...      111335.00   \n",
            "1  https://coin-images.coingecko.com/coins/images...        4510.85   \n",
            "2  https://coin-images.coingecko.com/coins/images...           2.98   \n",
            "3  https://coin-images.coingecko.com/coins/images...           1.00   \n",
            "4  https://coin-images.coingecko.com/coins/images...         856.33   \n",
            "\n",
            "      market_cap  market_cap_rank  fully_diluted_valuation  total_volume  \\\n",
            "0  2216112820801                1            2216112820801   38032879153   \n",
            "1   544090381581                2             544090381581   36003577227   \n",
            "2   177301473452                3             298032251412    6165646145   \n",
            "3   167223859955                4             167223859955   84702498729   \n",
            "4   119267296969                5             119267296969    1273540825   \n",
            "\n",
            "    high_24h  ...  total_supply    max_supply        ath  \\\n",
            "0  112555.00  ...  1.991244e+07  2.100000e+07  124128.00   \n",
            "1    4657.28  ...  1.207069e+08           NaN    4946.05   \n",
            "2       3.04  ...  9.998582e+10  1.000000e+11       3.65   \n",
            "3       1.00  ...  1.672055e+11           NaN       1.32   \n",
            "4     866.62  ...  1.392872e+08  2.000000e+08     899.77   \n",
            "\n",
            "   ath_change_percentage                  ath_date        atl  \\\n",
            "0              -10.35157  2025-08-14T00:37:02.582Z  67.810000   \n",
            "1               -8.81036  2025-08-24T19:21:03.333Z   0.432979   \n",
            "2              -18.29388  2025-07-18T03:40:53.808Z   0.002686   \n",
            "3              -24.41177  2018-07-24T00:00:00.000Z   0.572521   \n",
            "4               -4.81976  2025-08-22T23:42:55.728Z   0.039818   \n",
            "\n",
            "   atl_change_percentage                  atl_date  \\\n",
            "0           1.640063e+05  2013-07-06T00:00:00.000Z   \n",
            "1           1.041587e+06  2015-10-20T00:00:00.000Z   \n",
            "2           1.108116e+05  2014-05-22T00:00:00.000Z   \n",
            "3           7.468430e+01  2015-03-02T00:00:00.000Z   \n",
            "4           2.150709e+06  2017-10-19T00:00:00.000Z   \n",
            "\n",
            "                                                 roi              last_updated  \n",
            "0                                               None  2025-08-27T22:25:42.267Z  \n",
            "1  {'times': 53.167263762065275, 'currency': 'btc...  2025-08-27T22:25:34.498Z  \n",
            "2                                               None  2025-08-27T22:25:36.346Z  \n",
            "3                                               None  2025-08-27T22:25:36.752Z  \n",
            "4                                               None  2025-08-27T22:25:37.881Z  \n",
            "\n",
            "[5 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f95c2c78",
        "outputId": "fc0948b8-d950-4a6c-fd46-1c9de480342d"
      },
      "source": [
        "# ----------------------------------------------------\n",
        "#---------Inicialización de la Sesión de Spark--------\n",
        "# ----------------------------------------------------\n",
        "# Crear una SparkSession, que es el punto de entrada para todas las funcionalidades de Spark.\n",
        "# 'appName' define el nombre de nuestra aplicación que aparecerá en la UI del cluster.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AnalisisCriptomonedas\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Configurar el nivel de log para que solo se muestren errores, evitando advertencias innecesarias.\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# -------Carga de Datos en un DataFrame de Spark------\n",
        "# ----------------------------------------------------\n",
        "# Crear un DataFrame de Spark a partir del DataFrame de Pandas que obtuvimos de la API.\n",
        "# Esto es útil para datos que ya están en memoria en nuestro driver program.\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "# Imprimir el esquema del DataFrame. Es crucial para entender la estructura de los datos:\n",
        "# los nombres de las columnas y sus tipos (StringType, DoubleType, etc.).\n",
        "print(\"Esquema del DataFrame de Spark:\")\n",
        "df.printSchema()\n",
        "\n",
        "# Mostrar las primeras 5 filas del DataFrame para una inspección visual rápida.\n",
        "# 'truncate=False' asegura que se muestren todos los caracteres de cada celda.\n",
        "print(\"Muestra de los datos crudos (primeras 5 filas):\")\n",
        "df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esquema del DataFrame de Spark:\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- symbol: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- image: string (nullable = true)\n",
            " |-- current_price: double (nullable = true)\n",
            " |-- market_cap: long (nullable = true)\n",
            " |-- market_cap_rank: long (nullable = true)\n",
            " |-- fully_diluted_valuation: long (nullable = true)\n",
            " |-- total_volume: long (nullable = true)\n",
            " |-- high_24h: double (nullable = true)\n",
            " |-- low_24h: double (nullable = true)\n",
            " |-- price_change_24h: double (nullable = true)\n",
            " |-- price_change_percentage_24h: double (nullable = true)\n",
            " |-- market_cap_change_24h: double (nullable = true)\n",
            " |-- market_cap_change_percentage_24h: double (nullable = true)\n",
            " |-- circulating_supply: double (nullable = true)\n",
            " |-- total_supply: double (nullable = true)\n",
            " |-- max_supply: double (nullable = true)\n",
            " |-- ath: double (nullable = true)\n",
            " |-- ath_change_percentage: double (nullable = true)\n",
            " |-- ath_date: string (nullable = true)\n",
            " |-- atl: double (nullable = true)\n",
            " |-- atl_change_percentage: double (nullable = true)\n",
            " |-- atl_date: string (nullable = true)\n",
            " |-- roi: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: double (valueContainsNull = true)\n",
            " |-- last_updated: string (nullable = true)\n",
            "\n",
            "Muestra de los datos crudos (primeras 5 filas):\n",
            "+-----------+------+--------+-------------------------------------------------------------------------------------------+-------------+-------------+---------------+-----------------------+------------+--------+--------+---------------------+---------------------------+----------------------+--------------------------------+--------------------+--------------------+----------+--------+---------------------+------------------------+----------+---------------------+------------------------+--------------------------------------------------------------------------------+------------------------+\n",
            "|id         |symbol|name    |image                                                                                      |current_price|market_cap   |market_cap_rank|fully_diluted_valuation|total_volume|high_24h|low_24h |price_change_24h     |price_change_percentage_24h|market_cap_change_24h |market_cap_change_percentage_24h|circulating_supply  |total_supply        |max_supply|ath     |ath_change_percentage|ath_date                |atl       |atl_change_percentage|atl_date                |roi                                                                             |last_updated            |\n",
            "+-----------+------+--------+-------------------------------------------------------------------------------------------+-------------+-------------+---------------+-----------------------+------------+--------+--------+---------------------+---------------------------+----------------------+--------------------------------+--------------------+--------------------+----------+--------+---------------------+------------------------+----------+---------------------+------------------------+--------------------------------------------------------------------------------+------------------------+\n",
            "|bitcoin    |btc   |Bitcoin |https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400              |111335.0     |2216112820801|1              |2216112820801          |38032879153 |112555.0|110464.0|-461.2837389549095   |-0.41261                   |-9.654080837193848E9  |-0.43374                        |1.9912443E7         |1.9912443E7         |2.1E7     |124128.0|-10.35157            |2025-08-14T00:37:02.582Z|67.81     |164006.30389         |2013-07-06T00:00:00.000Z|NULL                                                                            |2025-08-27T22:25:42.267Z|\n",
            "|ethereum   |eth   |Ethereum|https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628           |4510.85      |544090381581 |2              |544090381581           |36003577227 |4657.28 |4492.92 |-79.45449862128862   |-1.73092                   |-1.0457792001914062E10|-1.88582                        |1.207069175715764E8 |1.207069175715764E8 |NaN       |4946.05 |-8.81036             |2025-08-24T19:21:03.333Z|0.432979  |1041586.83306        |2015-10-20T00:00:00.000Z|{times -> 53.167263762065275, currency -> NULL, percentage -> 5316.726376206528}|2025-08-27T22:25:34.498Z|\n",
            "|ripple     |xrp   |XRP     |https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442|2.98         |177301473452 |3              |298032251412           |6165646145  |3.04    |2.97    |-0.038258810766949924|-1.26705                   |-2.2014389955845337E9 |-1.22641                        |5.9482264023E10     |9.9985819185E10     |1.0E11    |3.65    |-18.29388            |2025-07-18T03:40:53.808Z|0.00268621|110811.56594         |2014-05-22T00:00:00.000Z|NULL                                                                            |2025-08-27T22:25:36.346Z|\n",
            "|tether     |usdt  |Tether  |https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661             |1.0          |167223859955 |4              |167223859955           |84702498729 |1.0     |1.0     |-5.5148224921364E-5  |-0.00551                   |-2742021.734802246    |-0.00164                        |1.672055092165589E11|1.672055092165589E11|NaN       |1.32    |-24.41177            |2018-07-24T00:00:00.000Z|0.572521  |74.6843              |2015-03-02T00:00:00.000Z|NULL                                                                            |2025-08-27T22:25:36.752Z|\n",
            "|binancecoin|bnb   |BNB     |https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970       |856.33       |119267296969 |5              |119267296969           |1273540825  |866.62  |853.7   |-7.543757309683201   |-0.87325                   |-1.0146288249449768E9 |-0.84354                        |1.3928719538E8      |1.3928719538E8      |2.0E8     |899.77  |-4.81976             |2025-08-22T23:42:55.728Z|0.0398177 |2150709.44974        |2017-10-19T00:00:00.000Z|NULL                                                                            |2025-08-27T22:25:37.881Z|\n",
            "+-----------+------+--------+-------------------------------------------------------------------------------------------+-------------+-------------+---------------+-----------------------+------------+--------+--------+---------------------+---------------------------+----------------------+--------------------------------+--------------------+--------------------+----------+--------+---------------------+------------------------+----------+---------------------+------------------------+--------------------------------------------------------------------------------+------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. **Procesamiento de datos en Spark**:\n",
        "\n",
        "*   Aplicar al menos tres transformaciones sobre los datos (ejemplo: filtrado, agrupación, mapeo,\n",
        "conversión de tipos, etc.).\n",
        "*   Aplicar al menos dos acciones para obtener resultados procesados (ejemplo: mostrar datos, contar\n",
        "registros, guardar resultados en un archivo, etc.)."
      ],
      "metadata": {
        "id": "uCPTxmpMs5tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------\n",
        "# --------------Transformaciones---------------\n",
        "# ---------------------------------------------\n",
        "print(\"\\n--- Aplicando Transformaciones ---\")\n",
        "\n",
        "# Transformación 1: Selección y Renombrado de Columnas.\n",
        "# Propósito: Crear un nuevo DataFrame con solo las columnas relevantes para nuestro análisis,\n",
        "# utilizando nombres más descriptivos y fáciles de usar.\n",
        "df_selected = df.select(\n",
        "    col(\"id\").alias(\"Cripto_ID\"),\n",
        "    col(\"symbol\").alias(\"Simbolo\"),\n",
        "    col(\"name\").alias(\"Nombre\"),\n",
        "    col(\"current_price\").alias(\"Precio_Actual_USD\"),\n",
        "    col(\"price_change_percentage_24h\").alias(\"Cambio_24h_%\"),\n",
        "    col(\"market_cap\").alias(\"Capitalizacion_Mercado_USD\")\n",
        ")\n",
        "print(\"Transformación 1 aplicada: Selección de columnas.\")\n",
        "df_selected.show(5)\n",
        "\n",
        "# Transformación 2: Filtrado.\n",
        "# Propósito: Aplicar un filtro para encontrar sólo las criptomonedas que han tenido\n",
        "# una ganancia en las últimas 24 horas (cambio porcentual > 0).\n",
        "df_ganancias = df_selected.filter(col(\"Cambio_24h_%\") > 0)\n",
        "print(\"Transformación 2 aplicada: Filtrado por ganancias.\")\n",
        "df_ganancias.show()\n",
        "\n",
        "# Transformación 3: Conversión de Tipo de Datos y Cálculo.\n",
        "# Propósito: La capitalización de mercado viene como un número muy largo (double).\n",
        "# La convertimos a billones (trillions en escala corta) de USD para que sea más legible.\n",
        "# Creamos una nueva columna 'Market_Cap_Trillions' con este valor calculado.\n",
        "df_final = df_ganancias.withColumn(\n",
        "    \"Capitalizacion_Miles_Millones\",\n",
        "    round(col(\"Capitalizacion_Mercado_USD\") / 1e9, 2)  # Convertir y redondear a 2 decimales\n",
        ").withColumn(\n",
        "    \"Cap_Mercado_Formateada\",\n",
        "    format_number(col(\"Capitalizacion_Miles_Millones\"), 0)  # Formatear con separadores de miles\n",
        ")\n",
        "print(\"Transformación 3 aplicada: Conversión a miles de millones con formateo.\")\n",
        "df_final.select(\n",
        "    \"Nombre\",\n",
        "    \"Precio_Actual_USD\",\n",
        "    \"Cambio_24h_%\",\n",
        "    \"Capitalizacion_Miles_Millones\",\n",
        "    \"Cap_Mercado_Formateada\"\n",
        ").show(truncate=False)\n",
        "\n",
        "# -----------------------------------------\n",
        "# ----------------- Acciones --------------\n",
        "# -----------------------------------------\n",
        "print(\"\\n--- Aplicando Acciones ---\")\n",
        "\n",
        "# Acción 1: count() - Desencadena un job de Spark para calcular y traer el número total de filas\n",
        "# en el DataFrame resultante (las criptomonedas en ganancia) al driver program.\n",
        "conteo = df_final.count()\n",
        "print(f\"Acción 1 (count): Número de criptomonedas en ganancia: {conteo}\")\n",
        "\n",
        "# Acción 2: show() - Desencadena un job para recolectar los datos y mostrarlos.\n",
        "# Ya hemos usado show() arriba, pero es la acción principal para ver resultados.\n",
        "print(\"Acción 2 (show): Mostrando el resultado final ordenado por ganancia:\")\n",
        "df_final.orderBy(col(\"Cambio_24h_%\").desc()).show()\n",
        "\n",
        "# Acción 3: Escribir resultados - Una acción muy común es guardar el resultado\n",
        "# en un archivo (por ejemplo, para ser consumido por otro sistema).\n",
        "# Esto escribe el DataFrame como un único archivo CSV en la carpeta 'resultados_cripto'.\n",
        "print(\"Acción 3 (write): Guardando resultados en '/resultados_cripto'...\")\n",
        "df_final.write.mode(\"overwrite\").format(\"csv\").option(\"header\", \"true\").save(\"resultados_cripto\")\n",
        "print(\"¡Escritura completada!\")\n",
        "\n",
        "# ----------------------------------------------\n",
        "# ------------------ Finalización --------------\n",
        "# ----------------------------------------------\n",
        "# Es una buena práctica detener la SparkSession para liberar todos los recursos asociados.\n",
        "spark.stop()\n",
        "print(\"Sesión de Spark finalizada.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3HmkztWtJ0H",
        "outputId": "28a2b8c0-28a8-4f0a-dda3-3dc5e7fe7d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Aplicando Transformaciones ---\n",
            "Transformación 1 aplicada: Selección de columnas.\n",
            "+-----------+-------+--------+-----------------+------------+--------------------------+\n",
            "|  Cripto_ID|Simbolo|  Nombre|Precio_Actual_USD|Cambio_24h_%|Capitalizacion_Mercado_USD|\n",
            "+-----------+-------+--------+-----------------+------------+--------------------------+\n",
            "|    bitcoin|    btc| Bitcoin|         111335.0|    -0.41261|             2216112820801|\n",
            "|   ethereum|    eth|Ethereum|          4510.85|    -1.73092|              544090381581|\n",
            "|     ripple|    xrp|     XRP|             2.98|    -1.26705|              177301473452|\n",
            "|     tether|   usdt|  Tether|              1.0|    -0.00551|              167223859955|\n",
            "|binancecoin|    bnb|     BNB|           856.33|    -0.87325|              119267296969|\n",
            "+-----------+-------+--------+-----------------+------------+--------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Transformación 2 aplicada: Filtrado por ganancias.\n",
            "+---------+-------+------+-----------------+------------+--------------------------+\n",
            "|Cripto_ID|Simbolo|Nombre|Precio_Actual_USD|Cambio_24h_%|Capitalizacion_Mercado_USD|\n",
            "+---------+-------+------+-----------------+------------+--------------------------+\n",
            "|   solana|    sol|Solana|           204.58|     3.93662|              110508937563|\n",
            "| usd-coin|   usdc|  USDC|         0.999808|      2.1E-4|               69347998222|\n",
            "+---------+-------+------+-----------------+------------+--------------------------+\n",
            "\n",
            "Transformación 3 aplicada: Conversión a miles de millones con formateo.\n",
            "+------+-----------------+------------+-----------------------------+----------------------+\n",
            "|Nombre|Precio_Actual_USD|Cambio_24h_%|Capitalizacion_Miles_Millones|Cap_Mercado_Formateada|\n",
            "+------+-----------------+------------+-----------------------------+----------------------+\n",
            "|Solana|204.58           |3.93662     |110.51                       |111                   |\n",
            "|USDC  |0.999808         |2.1E-4      |69.35                        |69                    |\n",
            "+------+-----------------+------------+-----------------------------+----------------------+\n",
            "\n",
            "\n",
            "--- Aplicando Acciones ---\n",
            "Acción 1 (count): Número de criptomonedas en ganancia: 2\n",
            "Acción 2 (show): Mostrando el resultado final ordenado por ganancia:\n",
            "+---------+-------+------+-----------------+------------+--------------------------+-----------------------------+----------------------+\n",
            "|Cripto_ID|Simbolo|Nombre|Precio_Actual_USD|Cambio_24h_%|Capitalizacion_Mercado_USD|Capitalizacion_Miles_Millones|Cap_Mercado_Formateada|\n",
            "+---------+-------+------+-----------------+------------+--------------------------+-----------------------------+----------------------+\n",
            "|   solana|    sol|Solana|           204.58|     3.93662|              110508937563|                       110.51|                   111|\n",
            "| usd-coin|   usdc|  USDC|         0.999808|      2.1E-4|               69347998222|                        69.35|                    69|\n",
            "+---------+-------+------+-----------------+------------+--------------------------+-----------------------------+----------------------+\n",
            "\n",
            "Acción 3 (write): Guardando resultados en '/resultados_cripto'...\n",
            "¡Escritura completada!\n",
            "Sesión de Spark finalizada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. **Explicación del código**:\n",
        "\n",
        "*   Incluir comentarios en el código para describir el propósito de cada bloque de instrucciones.\n",
        "*   Adjuntar un breve informe explicando el flujo del procesamiento de datos en Spark."
      ],
      "metadata": {
        "id": "q-bgkhu7zvQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicación del Flujo de Procesamiento\n",
        "\n",
        "**Inicialización:** Todo comienza con la SparkSession, el coordinador de nuestro trabajo en Spark.\n",
        "\n",
        "**Carga de Datos:** Los datos JSON obtenidos de la API se convierten en un DataFrame de Spark, la estructura de datos fundamental organizada en columnas.\n",
        "\n",
        "**Transformaciones:** Definimos una serie de operaciones:\n",
        "- `select()`: Especificamos qué columnas nos interesan y les damos nombres claros.\n",
        "- `filter()`: Eliminamos filas que no cumplen con nuestro criterio (criptos con pérdidas).\n",
        "- `withColumn()`: Creamos una nueva columna derivada de una existente (cálculo en miles de millones).\n",
        "\n",
        "Estas transformaciones son **perezosas (lazy)**. Spark construye un **plan lógico (Directed Acyclic Graph - DAG)** pero no ejecuta ningún cálculo real hasta que se llama a una acción.\n",
        "\n",
        "**Acciones:** Las acciones como `count()`, `show()`, y `write()` son las que **desencadenan la ejecución real** del plan lógico. Spark divide el trabajo en **tareas** que son distribuidas y ejecutadas en paralelo en el cluster (o en tu máquina local).\n",
        "\n",
        "**Resultado:** El producto final es un conjunto de datos transformado que mostramos en pantalla y guardamos en disco, obteniendo insights de los datos originales."
      ],
      "metadata": {
        "id": "rLDq0xxQz6Hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. **Análisis de opciones en la nube**:\n",
        "\n",
        "*   Investigar y seleccionar un proveedor de cloud computing (Amazon Web Services, Microsoft Azure,\n",
        "Google Cloud, etc.) que permita ejecutar Apache Spark.\n",
        "*   Analizar y comparar las características del servicio, incluyendo costos, ventajas y limitaciones.\n",
        "*   Presentar la información en un cuadro comparativo o en un documento estructurado."
      ],
      "metadata": {
        "id": "VYfOEYKF0_Rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proveedor Seleccionado:** Amazon Web Services (AWS)\n",
        "\n",
        "*Servicio Principal para Spark: Amazon EMR (Elastic MapReduce)*\n",
        "\n",
        "EMR es un servicio gestionado de cluster que simplifica enormemente la ejecución de frameworks de procesamiento de datos distribuidos como Apache Spark, Hadoop, Hive, y Presto."
      ],
      "metadata": {
        "id": "pwDdic_v1kBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Análisis Comparativo de Amazon EMR\n",
        "from IPython.display import HTML\n",
        "\n",
        "html_table = \"\"\"\n",
        "<table border=\"1\" style=\"border-collapse: collapse; width: 100%;\">\n",
        "<thead>\n",
        "<tr style=\"background-color: #f2f2f2;\">\n",
        "<th style=\"padding: 8px; text-align: left;\">Característica</th>\n",
        "<th style=\"padding: 8px; text-align: left;\">Descripción en AWS EMR</th>\n",
        "<th style=\"padding: 8px; text-align: left;\">Ventajas</th>\n",
        "<th style=\"padding: 8px; text-align: left;\">Limitaciones</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td style=\"padding: 8px;\"><strong>Configuración</strong></td>\n",
        "<td style=\"padding: 8px;\">Totalmente gestionado. Puedes crear un cluster Spark en minutos desde la consola, CLI o SDK.</td>\n",
        "<td style=\"padding: 8px;\"><strong>Ahorro de tiempo:</strong> Sin necesidad de instalar, configurar o mantener software manualmente. Escalado automático.</td>\n",
        "<td style=\"padding: 8px;\">Menor control detallado sobre la configuración del cluster en comparación con una configuración auto-gestionada en EC2.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 8px;\"><strong>Costo</strong></td>\n",
        "<td style=\"padding: 8px;\">Modelo de pago por uso. Se paga por las instancias EC2 y el almacenamiento EBS utilizadas. El costo del servicio EMR es adicional por hora.</td>\n",
        "<td style=\"padding: 8px;\"><strong>Coste-efectivo:</strong> Puedes usar <strong>Spot Instances</strong> para cargas de trabajo tolerantes a fallos con descuentos de hasta 90%. Sólo pagas mientras el cluster esté corriendo.</td>\n",
        "<td style=\"padding: 8px;\">Puede volverse costoso si los clusters se dejan corriendo de forma inadvertida (\"cluster sprawl\"). Los precios de las instancias EC2 varían por región.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 8px;\"><strong>Rendimiento</strong></td>\n",
        "<td style=\"padding: 8px;\">Optimizado para cargas de datos grandes. Se integra profundamente con otros servicios AWS como S3 (almacenamiento), Glacier, y Redshift (data warehouse).</td>\n",
        "<td style=\"padding: 8px;\"><strong>Alto rendimiento:</strong> Instancias optimizadas para computación y memoria. Lectura/escritura directa y muy rápida desde/hacia Amazon S3.</td>\n",
        "<td style=\"padding: 8px;\">La latencia de red puede ser un factor si tus datos de origen no están en AWS.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 8px;\"><strong>Escalabilidad</strong></td>\n",
        "<td style=\"padding: 8px;\">Escalado vertical (aumentar el tamaño de la instancia) y horizontal (añadir más nodos al cluster) de forma automática o manual.</td>\n",
        "<td style=\"padding: 8px;\"><strong>Escalado elástico:</strong> Puedes empezar con un solo nodo y escalar a cientos de instancias según la carga.</td>\n",
        "<td style=\"padding: 8px;\">El escalado automático requiere configuración y monitoreo para optimizar costos/rendimiento.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 8px;\"><strong>Confiabilidad</strong></td>\n",
        "<td style=\"padding: 8px;\">Alta disponibilidad integrada. Puede configurar clusters multi-AZ (Availability Zones). Reintentos automáticos de tareas fallidas.</td>\n",
        "<td style=\"padding: 8px;\"><strong>Gestionado:</strong> AWS se encarga de la salud de los nodos, reemplazándolos automáticamente si fallan.</td>\n",
        "<td style=\"padding: 8px;\">La terminación de Spot Instances puede interrumpir jobs largos si no se maneja correctamente.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"padding: 8px;\"><strong>Seguridad</strong></td>\n",
        "<td style=\"padding: 8px;\">Integración con IAM (Identity and Access Management) para control de acceso. Encriptación de datos en tránsito y en reposo. VPC networking.</td>\n",
        "<td style=\"padding: 8px;\"><strong>Seguridad empresarial:</strong> Cumple con varios estándares de compliance (PCI DSS, HIPAA, etc.).</td>\n",
        "<td style=\"padding: 8px;\">La configuración de seguridad (IAM, VPC) añade complejidad inicial.</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_table))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "cellView": "form",
        "id": "e6Fhyhy21qyf",
        "outputId": "26445b9e-3ef0-4904-e656-9608d703b643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<table border=\"1\" style=\"border-collapse: collapse; width: 100%;\">\n",
              "<thead>\n",
              "<tr style=\"background-color: #f2f2f2;\">\n",
              "<th style=\"padding: 8px; text-align: left;\">Característica</th>\n",
              "<th style=\"padding: 8px; text-align: left;\">Descripción en AWS EMR</th>\n",
              "<th style=\"padding: 8px; text-align: left;\">Ventajas</th>\n",
              "<th style=\"padding: 8px; text-align: left;\">Limitaciones</th>\n",
              "</tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr>\n",
              "<td style=\"padding: 8px;\"><strong>Configuración</strong></td>\n",
              "<td style=\"padding: 8px;\">Totalmente gestionado. Puedes crear un cluster Spark en minutos desde la consola, CLI o SDK.</td>\n",
              "<td style=\"padding: 8px;\"><strong>Ahorro de tiempo:</strong> Sin necesidad de instalar, configurar o mantener software manualmente. Escalado automático.</td>\n",
              "<td style=\"padding: 8px;\">Menor control detallado sobre la configuración del cluster en comparación con una configuración auto-gestionada en EC2.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td style=\"padding: 8px;\"><strong>Costo</strong></td>\n",
              "<td style=\"padding: 8px;\">Modelo de pago por uso. Se paga por las instancias EC2 y el almacenamiento EBS utilizadas. El costo del servicio EMR es adicional por hora.</td>\n",
              "<td style=\"padding: 8px;\"><strong>Coste-efectivo:</strong> Puedes usar <strong>Spot Instances</strong> para cargas de trabajo tolerantes a fallos con descuentos de hasta 90%. Sólo pagas mientras el cluster esté corriendo.</td>\n",
              "<td style=\"padding: 8px;\">Puede volverse costoso si los clusters se dejan corriendo de forma inadvertida (\"cluster sprawl\"). Los precios de las instancias EC2 varían por región.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td style=\"padding: 8px;\"><strong>Rendimiento</strong></td>\n",
              "<td style=\"padding: 8px;\">Optimizado para cargas de datos grandes. Se integra profundamente con otros servicios AWS como S3 (almacenamiento), Glacier, y Redshift (data warehouse).</td>\n",
              "<td style=\"padding: 8px;\"><strong>Alto rendimiento:</strong> Instancias optimizadas para computación y memoria. Lectura/escritura directa y muy rápida desde/hacia Amazon S3.</td>\n",
              "<td style=\"padding: 8px;\">La latencia de red puede ser un factor si tus datos de origen no están en AWS.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td style=\"padding: 8px;\"><strong>Escalabilidad</strong></td>\n",
              "<td style=\"padding: 8px;\">Escalado vertical (aumentar el tamaño de la instancia) y horizontal (añadir más nodos al cluster) de forma automática o manual.</td>\n",
              "<td style=\"padding: 8px;\"><strong>Escalado elástico:</strong> Puedes empezar con un solo nodo y escalar a cientos de instancias según la carga.</td>\n",
              "<td style=\"padding: 8px;\">El escalado automático requiere configuración y monitoreo para optimizar costos/rendimiento.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td style=\"padding: 8px;\"><strong>Confiabilidad</strong></td>\n",
              "<td style=\"padding: 8px;\">Alta disponibilidad integrada. Puede configurar clusters multi-AZ (Availability Zones). Reintentos automáticos de tareas fallidas.</td>\n",
              "<td style=\"padding: 8px;\"><strong>Gestionado:</strong> AWS se encarga de la salud de los nodos, reemplazándolos automáticamente si fallan.</td>\n",
              "<td style=\"padding: 8px;\">La terminación de Spot Instances puede interrumpir jobs largos si no se maneja correctamente.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "<td style=\"padding: 8px;\"><strong>Seguridad</strong></td>\n",
              "<td style=\"padding: 8px;\">Integración con IAM (Identity and Access Management) para control de acceso. Encriptación de datos en tránsito y en reposo. VPC networking.</td>\n",
              "<td style=\"padding: 8px;\"><strong>Seguridad empresarial:</strong> Cumple con varios estándares de compliance (PCI DSS, HIPAA, etc.).</td>\n",
              "<td style=\"padding: 8px;\">La configuración de seguridad (IAM, VPC) añade complejidad inicial.</td>\n",
              "</tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}